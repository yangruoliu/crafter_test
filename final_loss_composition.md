# æœ€ç»ˆLossç»„æˆè¯¦è§£

## ğŸ¯ æœ€ç»ˆLosså…¬å¼

### ä¿®å¤åçš„Lossè®¡ç®— (è®­ç»ƒæ­¥éª¤â‰¥10æ—¶)
```python
final_loss = (adaptive_policy_weight * policy_loss_norm + 
              adaptive_entropy_weight * entropy_loss_norm + 
              adaptive_value_weight * value_loss_norm + 
              adaptive_direction_weight * direction_loss_norm)
```

### åˆå§‹é˜¶æ®µLossè®¡ç®— (è®­ç»ƒæ­¥éª¤<10æ—¶)
```python
final_loss = (policy_loss + 
              ent_coef * entropy_loss + 
              vf_coef * value_loss + 
              direction_weight * direction_loss)
```

## ğŸ§© Lossç»„æˆéƒ¨åˆ†è¯¦è§£

### 1. **Policy Loss (ç­–ç•¥æŸå¤±)**
```python
# PPOçš„æ ¸å¿ƒæŸå¤± - Clipped Surrogate Objective
ratio = torch.exp(log_prob - rollout_data.old_log_prob)
policy_loss_1 = advantages * ratio
policy_loss_2 = advantages * torch.clamp(ratio, 1-clip_range, 1+clip_range)
policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()
```

**å«ä¹‰**:
- **ç›®æ ‡**: æå‡ç­–ç•¥æ€§èƒ½ï¼Œè®©æ™ºèƒ½ä½“é€‰æ‹©æ›´å¥½çš„åŠ¨ä½œ
- **åŸç†**: åŸºäºä¼˜åŠ¿å‡½æ•°è°ƒæ•´åŠ¨ä½œæ¦‚ç‡
- **PPOç‰¹è‰²**: ä½¿ç”¨è£å‰ªé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
- **è´Ÿå·**: å› ä¸ºè¦æœ€å¤§åŒ–å¥–åŠ±ï¼Œæ‰€ä»¥æœ€å°åŒ–è´Ÿå¥–åŠ±

**æ•°å­¦ç›´è§‰**:
- `ratio`: æ–°ç­–ç•¥ç›¸å¯¹äºæ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”å€¼
- `advantages > 0`: å¥½åŠ¨ä½œï¼Œå¢åŠ æ¦‚ç‡
- `advantages < 0`: ååŠ¨ä½œï¼Œå‡å°‘æ¦‚ç‡
- `clip`: é˜²æ­¢å•æ¬¡æ›´æ–°è¿‡æ¿€è¿›

### 2. **Value Loss (ä»·å€¼å‡½æ•°æŸå¤±)**
```python
# å‡æ–¹è¯¯å·®æŸå¤±
value_loss = F.mse_loss(rollout_data.returns, values_pred)
```

**å«ä¹‰**:
- **ç›®æ ‡**: è®©ä»·å€¼å‡½æ•°å‡†ç¡®é¢„æµ‹æœªæ¥å¥–åŠ±
- **åŸç†**: æœ€å°åŒ–é¢„æµ‹ä»·å€¼ä¸å®é™…å›æŠ¥çš„å·®å¼‚
- **é‡è¦æ€§**: ä»·å€¼å‡½æ•°è´¨é‡ç›´æ¥å½±å“ä¼˜åŠ¿å‡½æ•°è®¡ç®—

**æ•°å­¦ç›´è§‰**:
- `returns`: å®é™…è·å¾—çš„æŠ˜æ‰£å¥–åŠ±æ€»å’Œ
- `values_pred`: ç¥ç»ç½‘ç»œé¢„æµ‹çš„çŠ¶æ€ä»·å€¼
- `MSE`: é¢„æµ‹è¶Šå‡†ç¡®ï¼Œlossè¶Šå°

### 3. **Entropy Loss (ç†µæŸå¤±)**
```python
# ç­–ç•¥ç†µ - é¼“åŠ±æ¢ç´¢
entropy = dist.entropy()
entropy_loss = -torch.mean(entropy)
```

**å«ä¹‰**:
- **ç›®æ ‡**: ç»´æŒç­–ç•¥çš„æ¢ç´¢æ€§ï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›
- **åŸç†**: ç†µé«˜=æ›´éšæœº=æ›´å¤šæ¢ç´¢
- **è´Ÿå·**: è¦æœ€å¤§åŒ–ç†µï¼Œæ‰€ä»¥æœ€å°åŒ–è´Ÿç†µ

**æ•°å­¦ç›´è§‰**:
- `entropy`: ç­–ç•¥çš„éšæœºæ€§ç¨‹åº¦
- `ç†µé«˜`: åŠ¨ä½œé€‰æ‹©æ›´å‡åŒ€ï¼Œæ¢ç´¢æ›´å¤š
- `ç†µä½`: åŠ¨ä½œé€‰æ‹©æ›´ç¡®å®šï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

### 4. **Direction Loss (æ–¹å‘é¢„æµ‹æŸå¤±)**
```python
# è¾…åŠ©ä»»åŠ¡ - äº¤å‰ç†µæŸå¤±
direction_loss = CrossEntropyLoss(direction_logits, direction_labels)
```

**å«ä¹‰**:
- **ç›®æ ‡**: å­¦ä¹ ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œé¢„æµ‹ç›®æ ‡ç‰©ä½“æ–¹å‘
- **åŸç†**: å°†æ–¹å‘é¢„æµ‹ä½œä¸ºè¾…åŠ©ç›‘ç£ä¿¡å·
- **å¥½å¤„**: å¸®åŠ©ç‰¹å¾æå–å™¨å­¦ä¹ ç©ºé—´è¡¨ç¤º

**9ä¸ªæ–¹å‘ç±»åˆ«**:
```
0: ä¸Šå·¦    1: ä¸Š      2: ä¸Šå³
3: å·¦      4: ä¸­å¿ƒ    5: å³  
6: ä¸‹å·¦    7: ä¸‹      8: ä¸‹å³
```

## âš–ï¸ æƒé‡ç³»ç»Ÿå¯¹æ¯”

### åŸå§‹æƒé‡ç³»ç»Ÿ (æœ‰é—®é¢˜)
```python
# å›ºå®šæƒé‡ï¼Œä¸è€ƒè™‘é‡çº§å·®å¼‚
loss = policy_loss +           # ~1.0
       0.01 * entropy_loss +   # ~-0.025 (è¢«æ·¹æ²¡)
       0.5 * value_loss +      # ~24.25 (ä¸»å¯¼)
       0.3 * direction_loss    # ~0.47 (è¢«å¿½ç•¥)
```

**é—®é¢˜**:
- Value lossä¸»å¯¼è®­ç»ƒ (24.25 >> å…¶ä»–)
- Entropyå’Œdirectionæƒé‡è¢«"æ·¹æ²¡"
- æƒé‡è®¾ç½®å¤±å»æ„ä¹‰

### å½’ä¸€åŒ–æƒé‡ç³»ç»Ÿ (ä¿®å¤å)
```python
# å…ˆå½’ä¸€åŒ–ï¼Œå†è‡ªé€‚åº”æƒé‡
policy_norm = policy_loss / |policy_ma|      # ~1.0
value_norm = value_loss / |value_ma|         # ~1.0  
entropy_norm = entropy_loss / |entropy_ma|  # ~1.0
direction_norm = direction_loss / |direction_ma| # ~1.0

# åŸºäºå˜å¼‚ç³»æ•°çš„è‡ªé€‚åº”æƒé‡
adaptive_weights = f(coefficient_of_variation)
```

**ä¼˜åŠ¿**:
- æ‰€æœ‰æŸå¤±åœ¨ç›¸åŒé‡çº§ (~1.0)
- æƒé‡è®¾ç½®å˜å¾—æœ‰æ„ä¹‰
- è‡ªåŠ¨é€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–

## ğŸ“Š å®é™…è®­ç»ƒä¸­çš„Lossæ¼”å˜

### é˜¶æ®µ1: åˆå§‹åŒ– (æ­¥éª¤1-10)
```python
# ä½¿ç”¨å›ºå®šæƒé‡å»ºç«‹ç»Ÿè®¡åŸºçº¿
loss = policy_loss + 0.02*entropy_loss + 0.3*value_loss + 0.2*direction_loss
```

### é˜¶æ®µ2: å½’ä¸€åŒ–è®­ç»ƒ (æ­¥éª¤11+)
```python
# ä½¿ç”¨è‡ªé€‚åº”å½’ä¸€åŒ–æƒé‡
loss = 0.25*policy_norm + 0.02*entropy_norm + 0.35*value_norm + 0.15*direction_norm
```

## ğŸ¯ å„Lossçš„è®­ç»ƒç›®æ ‡

| Lossç±»å‹ | è®­ç»ƒç›®æ ‡ | æˆåŠŸæŒ‡æ ‡ | 
|----------|----------|----------|
| **Policy** | æå‡ä»»åŠ¡æ€§èƒ½ | Episode rewardå¢åŠ  |
| **Value** | å‡†ç¡®ä»·å€¼ä¼°è®¡ | Explained varianceæ¥è¿‘1 |
| **Entropy** | ç»´æŒæ¢ç´¢ | ä¸è¿‡æ—©æ”¶æ•›ï¼Œç­–ç•¥å¤šæ ·æ€§ |
| **Direction** | ç©ºé—´æ„ŸçŸ¥ | Direction accuracyæå‡ |

## ğŸ” Lossç›‘æ§æŒ‡æ ‡

### åŸºç¡€æŸå¤±å€¼
- `train/policy_gradient_loss`: ç­–ç•¥æ¢¯åº¦æŸå¤±
- `train/value_loss`: ä»·å€¼å‡½æ•°æŸå¤±  
- `train/entropy_loss`: ç†µæŸå¤± (é€šå¸¸ä¸ºè´Ÿ)
- `train/direction_loss`: æ–¹å‘é¢„æµ‹æŸå¤±

### å½’ä¸€åŒ–ç»Ÿè®¡
- `train/loss_norm/*_ma`: å„æŸå¤±çš„ç§»åŠ¨å¹³å‡
- `train/loss_norm/*_cov`: å„æŸå¤±çš„å˜å¼‚ç³»æ•°

### è‡ªé€‚åº”æƒé‡
- `train/adaptive_weights/*`: åŠ¨æ€è°ƒæ•´çš„æƒé‡
- åº”è¯¥éƒ½ä¸ºæ­£å€¼ä¸”ç›¸å¯¹å¹³è¡¡

### æ€§èƒ½æŒ‡æ ‡
- `train/direction_accuracy`: æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡
- `rollout/ep_rew_mean`: å¹³å‡episodeå¥–åŠ±

## ğŸ›ï¸ è°ƒä¼˜å»ºè®®

### å¦‚æœæ–¹å‘é¢„æµ‹å­¦ä¸å¥½
- å¢åŠ  `direction_weight` (0.2 â†’ 0.3)
- æ£€æŸ¥æ–¹å‘æ ‡ç­¾æ˜¯å¦æ­£ç¡®

### å¦‚æœæ¢ç´¢ä¸è¶³
- å¢åŠ  `ent_coef` (0.02 â†’ 0.03)
- é™ä½ `norm_decay` è®©æƒé‡æ›´çµæ´»

### å¦‚æœè®­ç»ƒä¸ç¨³å®š
- å‡å°‘ `learning_rate`
- å¢åŠ  `norm_decay` è®©ç»Ÿè®¡æ›´å¹³æ»‘
- å‡å°‘å„æƒé‡ç³»æ•°

### å¦‚æœæ”¶æ•›å¤ªæ…¢
- é€‚åº¦å¢åŠ  `learning_rate`
- å¢åŠ  `n_epochs`
- è°ƒæ•´ `batch_size`

æœ€ç»ˆç›®æ ‡æ˜¯è®©å››ä¸ªæŸå¤±åè°ƒå·¥ä½œï¼Œæ—¢å®Œæˆä¸»è¦ä»»åŠ¡åˆè·å¾—è‰¯å¥½çš„è¾…åŠ©èƒ½åŠ›ï¼