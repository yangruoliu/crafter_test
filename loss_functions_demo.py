#!/usr/bin/env python3
"""
å››å¤§æŸå¤±å‡½æ•°è®¡ç®—æ¼”ç¤º
"""

import numpy as np
import torch
import torch.nn.functional as F

def demo_policy_loss():
    """æ¼”ç¤ºPolicy Lossè®¡ç®—"""
    print("ğŸ¯ Policy Loss è®¡ç®—æ¼”ç¤º")
    print("-" * 50)
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 4
    
    # ä¼˜åŠ¿å‡½æ•° (æ­£æ•°=å¥½åŠ¨ä½œï¼Œè´Ÿæ•°=ååŠ¨ä½œ)
    advantages = torch.tensor([2.5, -1.2, 0.8, -0.5])
    
    # æ—§ç­–ç•¥å’Œæ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
    old_log_probs = torch.tensor([-1.6, -0.9, -2.3, -1.1])  # log(æ¦‚ç‡)
    new_log_probs = torch.tensor([-1.2, -1.5, -1.8, -1.3])
    
    # è®¡ç®—æ¦‚ç‡æ¯”å€¼
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    print("è¾“å…¥æ•°æ®:")
    print(f"  ä¼˜åŠ¿å‡½æ•°: {advantages.numpy()}")
    print(f"  æ—§ç­–ç•¥logæ¦‚ç‡: {old_log_probs.numpy()}")
    print(f"  æ–°ç­–ç•¥logæ¦‚ç‡: {new_log_probs.numpy()}")
    print(f"  æ¦‚ç‡æ¯”å€¼: {ratio.numpy()}")
    
    # PPOè£å‰ª
    clip_range = 0.2
    ratio_clipped = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)
    
    # ä¸¤ç§æŸå¤±è®¡ç®—
    policy_loss_1 = advantages * ratio
    policy_loss_2 = advantages * ratio_clipped
    
    # å–è¾ƒå°å€¼ï¼ˆæ›´ä¿å®ˆï¼‰
    policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()
    
    print(f"\nPPOè®¡ç®—:")
    print(f"  è£å‰ªèŒƒå›´: [{1-clip_range:.1f}, {1+clip_range:.1f}]")
    print(f"  è£å‰ªåæ¯”å€¼: {ratio_clipped.numpy()}")
    print(f"  æŸå¤±1 (åŸå§‹): {policy_loss_1.numpy()}")
    print(f"  æŸå¤±2 (è£å‰ª): {policy_loss_2.numpy()}")
    print(f"  æœ€ç»ˆPolicy Loss: {policy_loss.item():.4f}")

def demo_value_loss():
    """æ¼”ç¤ºValue Lossè®¡ç®—"""
    print("\nğŸ“Š Value Loss è®¡ç®—æ¼”ç¤º")
    print("-" * 50)
    
    # æ¨¡æ‹ŸçŠ¶æ€ä»·å€¼é¢„æµ‹å’Œå®é™…å›æŠ¥
    predicted_values = torch.tensor([50.2, 78.5, 92.1, 15.3])
    actual_returns = torch.tensor([52.0, 85.2, 88.7, 18.1])
    
    print("è¾“å…¥æ•°æ®:")
    print(f"  é¢„æµ‹ä»·å€¼: {predicted_values.numpy()}")
    print(f"  å®é™…å›æŠ¥: {actual_returns.numpy()}")
    
    # è®¡ç®—MSEæŸå¤±
    value_loss = F.mse_loss(predicted_values, actual_returns)
    
    # è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    differences = predicted_values - actual_returns
    squared_errors = differences ** 2
    
    print(f"\nè®¡ç®—è¿‡ç¨‹:")
    print(f"  é¢„æµ‹è¯¯å·®: {differences.numpy()}")
    print(f"  å¹³æ–¹è¯¯å·®: {squared_errors.numpy()}")
    print(f"  å¹³å‡å¹³æ–¹è¯¯å·®: {squared_errors.mean().item():.4f}")
    print(f"  æœ€ç»ˆValue Loss: {value_loss.item():.4f}")

def demo_entropy_loss():
    """æ¼”ç¤ºEntropy Lossè®¡ç®—"""
    print("\nğŸ² Entropy Loss è®¡ç®—æ¼”ç¤º")
    print("-" * 50)
    
    # æ¨¡æ‹Ÿä¸åŒæ¢ç´¢ç¨‹åº¦çš„ç­–ç•¥
    print("æ¯”è¾ƒä¸¤ç§ç­–ç•¥çš„ç†µ:")
    
    # é«˜ç†µç­–ç•¥ (æ¢ç´¢æ€§å¼º)
    high_entropy_probs = torch.tensor([[0.25, 0.25, 0.25, 0.25],  # å‡åŒ€åˆ†å¸ƒ
                                      [0.3, 0.2, 0.3, 0.2]])
    
    # ä½ç†µç­–ç•¥ (ç¡®å®šæ€§å¼º)  
    low_entropy_probs = torch.tensor([[0.9, 0.05, 0.03, 0.02],   # åå‘ä¸€ä¸ªåŠ¨ä½œ
                                     [0.85, 0.1, 0.03, 0.02]])
    
    def calculate_entropy(probs):
        # ç†µ = -sum(p * log(p))
        log_probs = torch.log(probs + 1e-8)  # é¿å…log(0)
        entropy = -torch.sum(probs * log_probs, dim=1)
        return entropy
    
    high_entropy = calculate_entropy(high_entropy_probs)
    low_entropy = calculate_entropy(low_entropy_probs)
    
    print(f"é«˜ç†µç­–ç•¥æ¦‚ç‡: {high_entropy_probs.numpy()}")
    print(f"é«˜ç†µç­–ç•¥ç†µå€¼: {high_entropy.numpy()}")
    print(f"å¹³å‡ç†µ: {high_entropy.mean().item():.4f}")
    
    print(f"\nä½ç†µç­–ç•¥æ¦‚ç‡: {low_entropy_probs.numpy()}")
    print(f"ä½ç†µç­–ç•¥ç†µå€¼: {low_entropy.numpy()}")
    print(f"å¹³å‡ç†µ: {low_entropy.mean().item():.4f}")
    
    # ç†µæŸå¤± (è´Ÿç†µï¼Œå› ä¸ºè¦æœ€å¤§åŒ–ç†µ)
    high_entropy_loss = -high_entropy.mean()
    low_entropy_loss = -low_entropy.mean()
    
    print(f"\nç†µæŸå¤±:")
    print(f"  é«˜ç†µç­–ç•¥æŸå¤±: {high_entropy_loss.item():.4f} (è®­ç»ƒæ—©æœŸæœŸæœ›)")
    print(f"  ä½ç†µç­–ç•¥æŸå¤±: {low_entropy_loss.item():.4f} (è®­ç»ƒåæœŸ)")

def demo_direction_loss():
    """æ¼”ç¤ºDirection Lossè®¡ç®—"""
    print("\nğŸ§­ Direction Loss è®¡ç®—æ¼”ç¤º")
    print("-" * 50)
    
    # 9ä¸ªæ–¹å‘ç±»åˆ«
    direction_names = [
        "ä¸Šå·¦", "ä¸Š", "ä¸Šå³",
        "å·¦", "ä¸­å¿ƒ", "å³", 
        "ä¸‹å·¦", "ä¸‹", "ä¸‹å³"
    ]
    
    # æ¨¡æ‹Ÿé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    batch_size = 3
    num_classes = 9
    
    # çœŸå®æ–¹å‘æ ‡ç­¾
    true_directions = torch.tensor([2, 5, 7])  # ä¸Šå³, å³, ä¸‹
    
    # æ¨¡æ‹Ÿç½‘ç»œè¾“å‡ºçš„logits
    predicted_logits = torch.tensor([
        [-0.5, 0.2, 2.1, -1.0, 0.1, 0.3, -0.8, 0.0, -0.2],  # é¢„æµ‹ä¸Šå³(æ­£ç¡®)
        [0.1, -0.3, 0.2, -0.1, 0.5, 1.8, 0.0, -0.5, 0.1],   # é¢„æµ‹å³(æ­£ç¡®)
        [0.3, 0.8, -0.2, 0.1, -0.1, 0.0, 0.2, -0.1, 0.4]    # é¢„æµ‹ä¸Š(é”™è¯¯,åº”è¯¥æ˜¯ä¸‹)
    ])
    
    print("è¾“å…¥æ•°æ®:")
    print(f"  çœŸå®æ–¹å‘: {[direction_names[i] for i in true_directions]}")
    print(f"  é¢„æµ‹logitså½¢çŠ¶: {predicted_logits.shape}")
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    predicted_probs = F.softmax(predicted_logits, dim=1)
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    direction_loss = F.cross_entropy(predicted_logits, true_directions)
    
    print(f"\nè¯¦ç»†åˆ†æ:")
    for i in range(batch_size):
        true_idx = true_directions[i].item()
        pred_probs = predicted_probs[i]
        pred_idx = torch.argmax(pred_probs).item()
        
        print(f"  æ ·æœ¬ {i+1}:")
        print(f"    çœŸå®æ–¹å‘: {direction_names[true_idx]}")
        print(f"    é¢„æµ‹æ–¹å‘: {direction_names[pred_idx]}")
        print(f"    é¢„æµ‹æ¦‚ç‡: {pred_probs[true_idx].item():.4f}")
        print(f"    å•æ ·æœ¬æŸå¤±: {-torch.log(pred_probs[true_idx]).item():.4f}")
    
    print(f"\næœ€ç»ˆDirection Loss: {direction_loss.item():.4f}")

def demo_combined_loss():
    """æ¼”ç¤ºå››ä¸ªæŸå¤±çš„ç»„åˆ"""
    print("\nğŸ”— ç»„åˆæŸå¤±æ¼”ç¤º")
    print("-" * 50)
    
    # æ¨¡æ‹Ÿå››ä¸ªæŸå¤±å€¼
    policy_loss = 0.85
    value_loss = 15.2
    entropy_loss = -1.8
    direction_loss = 1.2
    
    print("åŸå§‹æŸå¤±å€¼ (ä¸åŒé‡çº§):")
    print(f"  Policy Loss:    {policy_loss:.3f}")
    print(f"  Value Loss:     {value_loss:.3f}  â† ä¸»å¯¼è®­ç»ƒ!")
    print(f"  Entropy Loss:   {entropy_loss:.3f}")
    print(f"  Direction Loss: {direction_loss:.3f}")
    
    # åŸå§‹æƒé‡ç³»ç»Ÿ
    original_weights = [1.0, 0.3, 0.02, 0.2]
    original_total = (policy_loss * original_weights[0] + 
                     value_loss * original_weights[1] + 
                     entropy_loss * original_weights[2] + 
                     direction_loss * original_weights[3])
    
    print(f"\nåŸå§‹åŠ æƒ (æœ‰é—®é¢˜):")
    print(f"  æ€»æŸå¤±: {original_total:.3f}")
    print(f"  Valueéƒ¨åˆ†è´¡çŒ®: {value_loss * original_weights[1]:.3f} (å ä¸»å¯¼)")
    
    # å½’ä¸€åŒ–ç³»ç»Ÿ
    moving_averages = [1.0, 48.5, -2.47, 1.58]
    normalized_losses = [
        policy_loss / abs(moving_averages[0]),
        value_loss / abs(moving_averages[1]), 
        entropy_loss / abs(moving_averages[2]),
        direction_loss / abs(moving_averages[3])
    ]
    
    adaptive_weights = [0.25, 0.35, 0.02, 0.15]  # åŸºäºå˜å¼‚ç³»æ•°
    
    normalized_total = sum(w * l for w, l in zip(adaptive_weights, normalized_losses))
    
    print(f"\nå½’ä¸€åŒ–ç³»ç»Ÿ (ä¿®å¤å):")
    print(f"  å½’ä¸€åŒ–æŸå¤±: {[f'{l:.3f}' for l in normalized_losses]}")
    print(f"  è‡ªé€‚åº”æƒé‡: {[f'{w:.3f}' for w in adaptive_weights]}")
    print(f"  æ€»æŸå¤±: {normalized_total:.3f}")
    print(f"  å„éƒ¨åˆ†å‡è¡¡å‚ä¸è®­ç»ƒ âœ“")

if __name__ == "__main__":
    print("ğŸ§© å››å¤§æŸå¤±å‡½æ•°è¯¦ç»†æ¼”ç¤º")
    print("=" * 70)
    
    demo_policy_loss()
    demo_value_loss()
    demo_entropy_loss()
    demo_direction_loss()
    demo_combined_loss()
    
    print("\nğŸ¯ æ€»ç»“")
    print("=" * 70)
    print("1. Policy Loss: å­¦ä¹ æ›´å¥½çš„å†³ç­–ç­–ç•¥ (PPOæ ¸å¿ƒ)")
    print("2. Value Loss: å‡†ç¡®ä¼°è®¡çŠ¶æ€ä»·å€¼ (æ”¯æŒç­–ç•¥å­¦ä¹ )")
    print("3. Entropy Loss: ç»´æŒæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ (é˜²æ­¢æ—©ç†Ÿ)")
    print("4. Direction Loss: å­¦ä¹ ç©ºé—´æ„ŸçŸ¥èƒ½åŠ› (è¾…åŠ©ä»»åŠ¡)")
    print("\nğŸ”§ æŸå¤±å½’ä¸€åŒ–ç¡®ä¿å››è€…åè°ƒå·¥ä½œï¼Œè€ŒéæŸä¸ªä¸»å¯¼è®­ç»ƒï¼")