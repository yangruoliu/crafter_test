#!/usr/bin/env python3
"""
ç´§æ€¥ä¿®å¤é…ç½® - V4
è§£å†³V3ä¸­Valueæƒé‡è¿‡åº¦ä¸»å¯¼çš„é—®é¢˜
"""

import gym
import crafter
import env_wrapper
from model_with_attn import CustomResNet, CustomACPolicy, CustomPPO, TQDMProgressBar
import torch.nn as nn
import torch
from stable_baselines3 import PPO
from gym.wrappers import FrameStack
import numpy as np
import os
from stable_baselines3.common.callbacks import BaseCallback
import time

timestamp = time.strftime("%Y%m%d_%H%M%S")

if __name__ == "__main__":

    config = {
        "total_timesteps": 100000,
        "save_dir": f"./stone_with_direction_v4_{timestamp}",
        "init_items": ["wood_pickaxe"],
        "init_num": [1],
        "target_obj_id": 7,  # Stone object ID
        "target_obj_name": "stone",
        "direction_weight": 0.8  # å¤§å¹…å¢åŠ æ–¹å‘é¢„æµ‹æƒé‡
    }

    env = gym.make("MyCrafter-v0") 

    # Apply wrappers in the correct order
    env = env_wrapper.MineStoneWrapper(env)
    env = env_wrapper.InitWrapper(env, init_items=config["init_items"], init_num=config["init_num"])
    
    # Add direction label wrapper - this should be the last wrapper to ensure
    # it gets the correct info from the environment
    env = env_wrapper.DirectionLabelWrapper(
        env, 
        target_obj_id=config["target_obj_id"], 
        target_obj_name=config["target_obj_name"]
    )

    policy_kwargs = {
        "features_extractor_class": CustomResNet,
        "features_extractor_kwargs": {"features_dim": 1024},
        "activation_fn": nn.ReLU,
        "net_arch": [],
        "optimizer_class": torch.optim.Adam,
        "optimizer_kwargs": {"eps": 1e-5},
        "num_aux_classes": 9  # 9 direction classes
    }

    # V4ç´§æ€¥ä¿®å¤é…ç½® - è§£å†³Valueæƒé‡è¿‡åº¦ä¸»å¯¼
    model = CustomPPO(
        CustomACPolicy,
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=2e-3,   # å¤§å¹…æé«˜å­¦ä¹ ç‡ï¼Œå¼ºåˆ¶ç­–ç•¥æ›´æ–°
        n_steps=4096,
        batch_size=512,
        n_epochs=3,           # é€‚åº¦å‡å°‘ï¼Œé˜²æ­¢Valueè¿‡æ‹Ÿåˆ
        gamma=0.95,
        gae_lambda=0.65,
        clip_range=0.2,
        ent_coef=0.2,         # å¤§å¹…å¢åŠ entropyï¼Œå¼ºåˆ¶æ¢ç´¢
        vf_coef=0.05,         # å¤§å¹…é™ä½valueæƒé‡ï¼Œé˜²æ­¢ä¸»å¯¼
        max_grad_norm=0.5,
        verbose=1,
        normalize_advantage=False,
        direction_weight=config["direction_weight"],  # å¤§å¹…å¢åŠ æ–¹å‘æƒé‡
        loss_normalization=True,  # å¯ç”¨å¸¦æƒé‡ä¸Šé™ä¿æŠ¤çš„æŸå¤±å½’ä¸€åŒ–
        norm_decay=0.7        # æ›´å¿«é€‚åº”å˜åŒ–
    )

    total_timesteps = config["total_timesteps"]

    print(f"ğŸš¨ å¯åŠ¨ç´§æ€¥ä¿®å¤é…ç½® V4")
    print(f"Target object: {config['target_obj_name']} (ID: {config['target_obj_id']})")
    print(f"Direction weight: {config['direction_weight']} (å¤§å¹…å¢åŠ )")
    print(f"Entropy coefficient: {model.ent_coef} (å¼ºåˆ¶æ¢ç´¢)")
    print(f"Value coefficient: {model.vf_coef} (å¤§å¹…é™ä½)")
    print(f"Learning rate: {model.learning_rate} (å¼ºåŒ–æ›´æ–°)")
    print(f"N epochs: {model.n_epochs} (é˜²æ­¢è¿‡æ‹Ÿåˆ)")
    print(f"Normalization decay: {model.norm_decay} (å¿«é€Ÿé€‚åº”)")
    print(f"Total timesteps: {total_timesteps}")
    print("=" * 70)
    
    print("ğŸš¨ V4ç´§æ€¥ä¿®å¤æªæ–½:")
    print("  - ä»£ç ä¿®å¤: æ·»åŠ æƒé‡ä¸Šé™ä¿æŠ¤ (max 40%)")
    print("  - ä»£ç ä¿®å¤: å¢å¼ºæœ€å°æƒé‡ä¿æŠ¤ (15%)")
    print("  - ä»£ç ä¿®å¤: æƒé‡é‡æ–°å½’ä¸€åŒ–æœºåˆ¶")
    print("  - å­¦ä¹ ç‡: 1e-3 â†’ 2e-3 (å¼ºåˆ¶ç­–ç•¥æ›´æ–°)")
    print("  - Entropyç³»æ•°: 0.12 â†’ 0.2 (å¼ºåˆ¶æ¢ç´¢)")
    print("  - Valueç³»æ•°: 0.3 â†’ 0.05 (å¤§å¹…é™ä½)")
    print("  - Directionæƒé‡: 0.6 â†’ 0.8 (å¤§å¹…å¢åŠ )")
    print("  - Norm decay: 0.8 â†’ 0.7 (æ›´å¿«é€‚åº”)")
    print("=" * 70)
    
    print("ğŸ¯ V4é¢„æœŸä¿®å¤æ•ˆæœ:")
    print("  - adaptive_weights/value < 40% (ä»28%å—é™)")
    print("  - adaptive_weights/policy > 15% (ä»1.14%)")
    print("  - adaptive_weights/entropy > 5% (ä»0.22%)")
    print("  - adaptive_weights/direction > 10% (ä»2.18%)")
    print("  - approx_kl > 1e-6 (ç­–ç•¥å¼€å§‹æ›´æ–°)")
    print("  - entropy_loss < -0.01 (çœŸå®æ¢ç´¢)")
    print("  - direction_accuracy > 35% (ä»29.4%)")
    print("=" * 70)

    model.learn(total_timesteps=total_timesteps, progress_bar=True)

    model.save(config["save_dir"])
    print(f"Model saved to {config['save_dir']}")

    env.close()

# V4ç‰ˆæœ¬ç´§æ€¥ä¿®å¤è¯´æ˜
"""
ğŸš¨ V4ä¸»è¦è§£å†³çš„é—®é¢˜:

1. Valueæƒé‡è¿‡åº¦ä¸»å¯¼ (V3ä¸­è¾¾åˆ°28%):
   - æ·»åŠ æƒé‡ä¸Šé™ä¿æŠ¤ (max 40%)
   - å¤§å¹…é™ä½vf_coef: 0.3 â†’ 0.05
   - æƒé‡é‡æ–°å½’ä¸€åŒ–æœºåˆ¶

2. æ¢ç´¢æœºåˆ¶å®Œå…¨å¤±æ•ˆ:
   - å¤§å¹…å¢åŠ ent_coef: 0.12 â†’ 0.2
   - å¢å¼ºæœ€å°æƒé‡ä¿æŠ¤: 0.1 â†’ 0.15

3. ç­–ç•¥æ›´æ–°åœæ»:
   - å¤§å¹…æé«˜learning_rate: 1e-3 â†’ 2e-3
   - é€‚åº¦å‡å°‘n_epochs: 4 â†’ 3

4. æ–¹å‘å­¦ä¹ ä¸è¶³:
   - è¿›ä¸€æ­¥å¢åŠ direction_weight: 0.6 â†’ 0.8

ğŸ¯ V3 â†’ V4 å…³é”®æ”¹è¿›:
- è§£å†³Value Function Overfitting
- æ¢å¤Policyå’ŒEntropyçš„ä¸»å¯¼åœ°ä½
- å¹³è¡¡å››ä¸ªæŸå¤±çš„æƒé‡åˆ†å¸ƒ
- é‡æ–°æ¿€æ´»ç­–ç•¥æ›´æ–°æœºåˆ¶

ğŸ“Š å¦‚æœV4ä»æœ‰é—®é¢˜:
- è€ƒè™‘è¿›ä¸€æ­¥é™ä½vf_coefåˆ°0.01
- å¢åŠ ent_coefåˆ°0.3
- æ£€æŸ¥ç¯å¢ƒå¥–åŠ±è®¾è®¡
- è€ƒè™‘é‡æ–°åˆå§‹åŒ–æ¨¡å‹
"""