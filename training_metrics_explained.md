# 训练输出指标详细解释

## 📊 Rollout指标 (游戏表现)

### `rollout/ep_len_mean`
- **含义**: 平均episode长度（每局游戏的步数）
- **你的值**: 168
- **正常范围**: 50-500 (取决于游戏设置)
- **解释**: 
  - 过短(<50): AI很快死亡或失败
  - 适中(100-300): AI能存活但可能表现一般
  - 过长(>500): 可能游戏没有明确结束条件
- **状态**: ✅ 正常，说明AI能在环境中存活一段时间

### `rollout/ep_rew_mean`
- **含义**: 平均episode奖励（每局游戏的总分）
- **你的值**: -5.94 ❌
- **正常范围**: 
  - 训练初期: -10 到 10
  - 训练中期: 10 到 100
  - 训练后期: 100+ (任务相关)
- **解释**: 
  - 负值: AI表现很差，可能在做错误动作
  - 零附近: AI表现平平
  - 正值且增长: AI正在学习
- **状态**: ❌ 严重问题，AI没有学会基本任务

---

## ⏱️ Time指标 (训练效率)

### `time/fps`
- **含义**: 每秒处理的帧数（训练速度）
- **你的值**: 92
- **正常范围**: 50-500 (取决于硬件)
- **解释**: 
  - <30: 训练很慢，可能硬件不足
  - 30-100: 一般速度
  - >100: 较快训练速度
- **状态**: ✅ 正常训练速度

### `time/iterations`
- **含义**: 完成的训练迭代次数
- **你的值**: 92
- **正常范围**: 无固定范围，随训练时间增长
- **解释**: 每次iteration会收集n_steps的数据并进行策略更新
- **状态**: ✅ 正常

### `time/total_timesteps`
- **含义**: 总训练步数（累计）
- **你的值**: 376,832
- **正常范围**: 无上限，越多表示训练时间越长
- **解释**: 376k步已经是相当长的训练时间
- **状态**: ⚠️ 训练时间足够但效果差

---

## 🔧 自适应权重 (损失平衡)

### `train/adaptive_weights/policy`
- **含义**: Policy loss在总损失中的动态权重
- **你的值**: 0.164 (16.4%)
- **正常范围**: 0.2-0.4 (20%-40%)
- **解释**: 控制策略学习的重要性
- **状态**: ⚠️ 略低，可以更高一些

### `train/adaptive_weights/value`
- **含义**: Value loss在总损失中的动态权重
- **你的值**: 0.144 (14.4%)
- **正常范围**: 0.1-0.3 (10%-30%)
- **解释**: 控制价值函数学习的重要性
- **状态**: ✅ 在正常范围内

### `train/adaptive_weights/entropy`
- **含义**: Entropy loss在总损失中的动态权重
- **你的值**: 0.00707 (0.7%) ❌
- **正常范围**: 0.02-0.1 (2%-10%)
- **解释**: 控制探索程度，过低导致探索不足
- **状态**: ❌ 严重过低，导致探索不足

### `train/adaptive_weights/direction`
- **含义**: Direction loss在总损失中的动态权重
- **你的值**: 0.000242 (0.02%) ❌
- **正常范围**: 0.05-0.2 (5%-20%)
- **解释**: 控制方向预测学习的重要性
- **状态**: ❌ 几乎为0，辅助任务失效

---

## 📈 核心训练指标

### `train/approx_kl`
- **含义**: 新旧策略间的KL散度（策略变化程度）
- **你的值**: 3.472087e-08 ❌
- **正常范围**: 1e-4 到 1e-2
- **解释**: 
  - 过小(<1e-6): 策略几乎不更新
  - 适中(1e-4到1e-2): 正常更新
  - 过大(>1e-1): 更新过激进
- **状态**: ❌ 极小，策略几乎停止学习

### `train/clip_fraction`
- **含义**: 被PPO裁剪的样本比例
- **你的值**: 0 ❌
- **正常范围**: 0.1-0.3
- **解释**: 
  - 0: 没有裁剪，更新很小
  - 0.1-0.3: 正常裁剪
  - >0.5: 更新过激进
- **状态**: ❌ 无裁剪，说明策略更新太小

### `train/learning_rate`
- **含义**: 当前学习率
- **你的值**: 0.0003
- **正常范围**: 1e-5 到 1e-3
- **解释**: 控制参数更新的步长
- **状态**: ✅ 在正常范围内

---

## 🎯 任务相关指标

### `train/direction_accuracy`
- **含义**: 方向预测的准确率
- **你的值**: 0.338 (33.8%)
- **正常范围**: 
  - 随机基线: 11.1% (1/9)
  - 学习中: 20%-50%
  - 良好: 50%+
- **解释**: 比随机好但还不够理想
- **状态**: ⚠️ 有改善但仍需提升

### `train/direction_loss`
- **含义**: 方向预测的交叉熵损失
- **你的值**: 1.75
- **正常范围**: 
  - 初期: 2.0-2.5 (接近随机)
  - 学习中: 1.0-2.0
  - 良好: <1.0
- **解释**: 中等水平，有下降空间
- **状态**: ⚠️ 可以更好

---

## 📊 损失函数指标

### `train/entropy_loss`
- **含义**: 策略熵损失（探索程度）
- **你的值**: -0.000512 ❌
- **正常范围**: -0.01 到 -2.0
- **解释**: 
  - 接近0: 几乎无探索
  - -0.01到-0.1: 低探索
  - -0.1到-2.0: 正常探索
- **状态**: ❌ 几乎无探索，严重问题

### `train/policy_gradient_loss`
- **含义**: 策略梯度损失
- **你的值**: 0.0893
- **正常范围**: 0.01-2.0
- **解释**: PPO的核心损失，衡量策略改进
- **状态**: ✅ 在正常范围内

### `train/value_loss`
- **含义**: 价值函数损失（预测准确性）
- **你的值**: 0.00233
- **正常范围**: 0.1-100（归一化前）
- **解释**: 很小可能因为归一化或价值函数训练不足
- **状态**: ⚠️ 可能需要更多训练

---

## 🔍 损失归一化统计

### `train/loss_norm/*_ma` (移动平均)
- **含义**: 各损失的历史移动平均值
- **你的值**: 
  - policy: 1.03 ✅
  - value: 0.00559 ⚠️ 
  - entropy: -0.00397 ❌
  - direction: 1.71 ✅
- **正常范围**: 通常为正值（entropy除外）
- **解释**: 用于归一化的基准值
- **状态**: entropy和value的移动平均异常小

### `train/loss_norm/*_cov` (变异系数)
- **含义**: 各损失的变异程度（标准差/平均值）
- **你的值**:
  - policy: 10.7 ⚠️
  - value: 31.3 ❌ 
  - entropy: 23.1 ❌
  - direction: 0.0788 ✅
- **正常范围**: 0.1-5.0
- **解释**: 
  - <1: 很稳定
  - 1-5: 正常波动
  - >10: 不稳定
- **状态**: value和entropy变异系数过高，训练不稳定

### `train/explained_variance`
- **含义**: 价值函数的解释方差（预测质量）
- **你的值**: -3.09 ❌
- **正常范围**: 0.0-1.0
- **解释**: 
  - 接近1: 价值函数预测很准确
  - 接近0: 预测一般
  - 负值: 预测很差，比常数预测还糟
- **状态**: ❌ 严重问题，价值函数质量很差

---

## 🎯 理想的训练指标应该是

```python
# 健康的训练指标示例
rollout/ep_rew_mean: 50+              # 正奖励且上升
train/adaptive_weights/policy: 0.3    # 30%
train/adaptive_weights/value: 0.2     # 20%  
train/adaptive_weights/entropy: 0.05  # 5%
train/adaptive_weights/direction: 0.1 # 10%
train/approx_kl: 0.001-0.01           # 适度更新
train/clip_fraction: 0.1-0.3          # 适度裁剪
train/direction_accuracy: 0.5+        # 50%+准确率
train/entropy_loss: -0.1 to -1.0      # 适度探索
train/explained_variance: 0.5+        # 价值函数质量好
train/loss_norm/*_cov: 0.5-3.0        # 稳定训练
```

## 🚨 你当前的主要问题

1. **探索完全停止**: entropy权重几乎为0
2. **方向学习失效**: direction权重几乎为0  
3. **策略停止更新**: KL散度极小
4. **价值函数很差**: explained_variance为负
5. **训练不稳定**: 多个损失变异系数过高
6. **任务表现差**: 负奖励

**建议**: 立即使用修复后的配置重新训练！