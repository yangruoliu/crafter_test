#!/usr/bin/env python3
"""
è®­ç»ƒåæ¨¡å‹æµ‹è¯•è„šæœ¬
æ”¯æŒå¤šç§æµ‹è¯•æ¨¡å¼å’Œè¯¦ç»†çš„æ€§èƒ½åˆ†æ
"""

import gym
import crafter
import env_wrapper
from model_with_attn import CustomResNet, CustomACPolicy, CustomPPO
import torch.nn as nn
import torch
import numpy as np
import os
import argparse
import time
from collections import defaultdict
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import json

class ModelTester:
    def __init__(self, model_path: str, render: bool = False):
        """
        åˆå§‹åŒ–æ¨¡å‹æµ‹è¯•å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            render: æ˜¯å¦æ¸²æŸ“æ¸¸æˆç”»é¢
        """
        self.model_path = model_path
        self.render = render
        self.model = None
        self.env = None
        self.test_results = defaultdict(list)
        
        self._setup_environment()
        self._load_model()
    
    def _setup_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
        
        # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„é…ç½®
        self.env = gym.make("MyCrafter-v0")
        
        # åº”ç”¨ä¸è®­ç»ƒç›¸åŒçš„wrapper
        self.env = env_wrapper.MineStoneWrapper(self.env)
        self.env = env_wrapper.InitWrapper(
            self.env, 
            init_items=["wood_pickaxe"], 
            init_num=[1]
        )
        self.env = env_wrapper.DirectionLabelWrapper(
            self.env, 
            target_obj_id=7,  # Stone object ID
            target_obj_name="stone"
        )
        
        print(f"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.env.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.env.action_space}")
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½æ¨¡å‹
            self.model = CustomPPO.load(self.model_path, env=self.env)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            print(f"   æ¨¡å‹ç±»å‹: {type(self.model).__name__}")
            if hasattr(self.model, 'loss_normalization'):
                print(f"   æŸå¤±å½’ä¸€åŒ–: {self.model.loss_normalization}")
            if hasattr(self.model, 'direction_weight'):
                print(f"   æ–¹å‘æƒé‡: {self.model.direction_weight}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def run_single_episode(self, max_steps: int = 1000, verbose: bool = True) -> Dict:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•å›åˆ
        
        Args:
            max_steps: æœ€å¤§æ­¥æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            å›åˆç»Ÿè®¡ä¿¡æ¯
        """
        obs = self.env.reset()
        done = False
        step = 0
        total_reward = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        actions_taken = []
        rewards_received = []
        direction_predictions = []
        direction_accuracies = []
        
        if verbose:
            print(f"\nğŸ® å¼€å§‹æ–°å›åˆ...")
        
        while not done and step < max_steps:
            # é¢„æµ‹åŠ¨ä½œ
            action, _states = self.model.predict(obs, deterministic=True)
            
            # å¦‚æœæ¨¡å‹æ”¯æŒæ–¹å‘é¢„æµ‹ï¼Œè·å–æ–¹å‘ä¿¡æ¯
            direction_pred = None
            direction_accuracy = None
            
            if hasattr(self.model.policy, 'predict_direction'):
                try:
                    with torch.no_grad():
                        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                        direction_logits = self.model.policy.predict_direction(obs_tensor)
                        direction_pred = torch.argmax(direction_logits, dim=1).item()
                        
                        # å¦‚æœç¯å¢ƒæä¾›çœŸå®æ–¹å‘æ ‡ç­¾
                        if hasattr(self.env, 'get_direction_label'):
                            true_direction = self.env.get_direction_label()
                            direction_accuracy = (direction_pred == true_direction)
                except:
                    pass
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = self.env.step(action)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            actions_taken.append(action)
            rewards_received.append(reward)
            if direction_pred is not None:
                direction_predictions.append(direction_pred)
            if direction_accuracy is not None:
                direction_accuracies.append(direction_accuracy)
            
            total_reward += reward
            step += 1
            
            if self.render:
                self.env.render()
                time.sleep(0.05)  # ç¨å¾®å‡æ…¢é€Ÿåº¦ä»¥ä¾¿è§‚å¯Ÿ
            
            if verbose and step % 100 == 0:
                print(f"   æ­¥æ•°: {step}, ç´¯è®¡å¥–åŠ±: {total_reward:.2f}")
        
        # ç¼–è¯‘å›åˆç»Ÿè®¡
        episode_stats = {
            'total_steps': step,
            'total_reward': total_reward,
            'average_reward': total_reward / step if step > 0 else 0,
            'actions_taken': actions_taken,
            'rewards_received': rewards_received,
            'final_info': info,
            'completed': done,
        }
        
        # æ·»åŠ æ–¹å‘é¢„æµ‹ç»Ÿè®¡
        if direction_predictions:
            episode_stats['direction_predictions'] = direction_predictions
            episode_stats['avg_direction_accuracy'] = np.mean(direction_accuracies) if direction_accuracies else 0
        
        if verbose:
            print(f"âœ… å›åˆç»“æŸ:")
            print(f"   æ€»æ­¥æ•°: {step}")
            print(f"   æ€»å¥–åŠ±: {total_reward:.2f}")
            print(f"   å¹³å‡å¥–åŠ±: {episode_stats['average_reward']:.3f}")
            if 'avg_direction_accuracy' in episode_stats:
                print(f"   æ–¹å‘å‡†ç¡®ç‡: {episode_stats['avg_direction_accuracy']:.3f}")
            print(f"   æ˜¯å¦å®Œæˆ: {done}")
        
        return episode_stats
    
    def run_multiple_episodes(self, num_episodes: int = 10, max_steps: int = 1000) -> Dict:
        """
        è¿è¡Œå¤šä¸ªæµ‹è¯•å›åˆå¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            num_episodes: æµ‹è¯•å›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
            
        Returns:
            æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\nğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯• ({num_episodes} å›åˆ)...")
        
        all_episode_stats = []
        
        for episode in range(num_episodes):
            print(f"\n--- å›åˆ {episode + 1}/{num_episodes} ---")
            
            episode_stats = self.run_single_episode(
                max_steps=max_steps, 
                verbose=(episode < 3)  # åªæ˜¾ç¤ºå‰3å›åˆçš„è¯¦ç»†ä¿¡æ¯
            )
            all_episode_stats.append(episode_stats)
            
            # ç®€è¦è¿›åº¦æŠ¥å‘Š
            if episode >= 3:
                print(f"å›åˆ {episode + 1}: å¥–åŠ±={episode_stats['total_reward']:.2f}, "
                      f"æ­¥æ•°={episode_stats['total_steps']}")
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        summary_stats = self._generate_summary_stats(all_episode_stats)
        
        return summary_stats
    
    def _generate_summary_stats(self, episode_stats: List[Dict]) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        # æå–å…³é”®æŒ‡æ ‡
        total_rewards = [ep['total_reward'] for ep in episode_stats]
        total_steps = [ep['total_steps'] for ep in episode_stats]
        avg_rewards = [ep['average_reward'] for ep in episode_stats]
        completion_rates = [ep['completed'] for ep in episode_stats]
        
        # æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡
        direction_accuracies = [
            ep['avg_direction_accuracy'] 
            for ep in episode_stats 
            if 'avg_direction_accuracy' in ep
        ]
        
        summary = {
            'num_episodes': len(episode_stats),
            'reward_stats': {
                'mean': np.mean(total_rewards),
                'std': np.std(total_rewards),
                'min': np.min(total_rewards),
                'max': np.max(total_rewards),
                'median': np.median(total_rewards)
            },
            'steps_stats': {
                'mean': np.mean(total_steps),
                'std': np.std(total_steps),
                'min': np.min(total_steps),
                'max': np.max(total_steps),
                'median': np.median(total_steps)
            },
            'completion_rate': np.mean(completion_rates),
            'avg_reward_per_step': {
                'mean': np.mean(avg_rewards),
                'std': np.std(avg_rewards)
            }
        }
        
        if direction_accuracies:
            summary['direction_accuracy'] = {
                'mean': np.mean(direction_accuracies),
                'std': np.std(direction_accuracies)
            }
        
        return summary
    
    def print_test_report(self, summary_stats: Dict):
        """æ‰“å°è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ¯ æ¨¡å‹æµ‹è¯•æŠ¥å‘Š")
        print("="*70)
        
        print(f"ğŸ“ˆ æµ‹è¯•æ¦‚å†µ:")
        print(f"   æµ‹è¯•å›åˆæ•°: {summary_stats['num_episodes']}")
        print(f"   ä»»åŠ¡å®Œæˆç‡: {summary_stats['completion_rate']:.1%}")
        
        print(f"\nğŸ† å¥–åŠ±ç»Ÿè®¡:")
        reward_stats = summary_stats['reward_stats']
        print(f"   å¹³å‡å¥–åŠ±: {reward_stats['mean']:.2f} Â± {reward_stats['std']:.2f}")
        print(f"   å¥–åŠ±èŒƒå›´: [{reward_stats['min']:.2f}, {reward_stats['max']:.2f}]")
        print(f"   ä¸­ä½æ•°å¥–åŠ±: {reward_stats['median']:.2f}")
        
        print(f"\nâ±ï¸ æ­¥æ•°ç»Ÿè®¡:")
        steps_stats = summary_stats['steps_stats']
        print(f"   å¹³å‡æ­¥æ•°: {steps_stats['mean']:.1f} Â± {steps_stats['std']:.1f}")
        print(f"   æ­¥æ•°èŒƒå›´: [{steps_stats['min']}, {steps_stats['max']}]")
        
        print(f"\nğŸ“Š æ•ˆç‡æŒ‡æ ‡:")
        efficiency = summary_stats['avg_reward_per_step']
        print(f"   å¹³å‡æ¯æ­¥å¥–åŠ±: {efficiency['mean']:.4f} Â± {efficiency['std']:.4f}")
        
        if 'direction_accuracy' in summary_stats:
            print(f"\nğŸ¯ æ–¹å‘é¢„æµ‹:")
            dir_acc = summary_stats['direction_accuracy']
            print(f"   å¹³å‡å‡†ç¡®ç‡: {dir_acc['mean']:.1%} Â± {dir_acc['std']:.1%}")
        
        # æ€§èƒ½è¯„çº§
        avg_reward = reward_stats['mean']
        completion_rate = summary_stats['completion_rate']
        
        print(f"\nğŸ… æ¨¡å‹æ€§èƒ½è¯„çº§:")
        if avg_reward > 50 and completion_rate > 0.8:
            grade = "A+ (ä¼˜ç§€)"
        elif avg_reward > 20 and completion_rate > 0.6:
            grade = "B+ (è‰¯å¥½)"
        elif avg_reward > 0 and completion_rate > 0.4:
            grade = "C+ (ä¸€èˆ¬)"
        elif avg_reward > -20:
            grade = "D (è¾ƒå·®)"
        else:
            grade = "F (å¤±è´¥)"
        
        print(f"   ç»¼åˆè¯„çº§: {grade}")
        
        print("="*70)
    
    def save_test_results(self, summary_stats: Dict, output_path: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        if output_path is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_path = f"test_results_{timestamp}.json"
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        test_report = {
            'model_path': self.model_path,
            'test_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'summary_stats': summary_stats
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def benchmark_performance(self, num_episodes: int = 20):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒçš„ç¡®å®šæ€§è®¾ç½®
        results = {}
        
        print(f"\næµ‹è¯•ç¡®å®šæ€§é¢„æµ‹...")
        start_time = time.time()
        summary_deterministic = self.run_multiple_episodes(
            num_episodes=num_episodes//2, 
            max_steps=500
        )
        deterministic_time = time.time() - start_time
        results['deterministic'] = {
            'stats': summary_deterministic,
            'avg_time_per_episode': deterministic_time / (num_episodes//2)
        }
        
        print(f"\nğŸ² æµ‹è¯•éšæœºæ€§é¢„æµ‹...")
        # ä¸´æ—¶ä¿®æ”¹æ¨¡å‹é¢„æµ‹æ–¹å¼
        original_predict = self.model.predict
        
        def stochastic_predict(obs, deterministic=False):
            return original_predict(obs, deterministic=False)
        
        self.model.predict = stochastic_predict
        
        start_time = time.time()
        summary_stochastic = self.run_multiple_episodes(
            num_episodes=num_episodes//2, 
            max_steps=500
        )
        stochastic_time = time.time() - start_time
        results['stochastic'] = {
            'stats': summary_stochastic,
            'avg_time_per_episode': stochastic_time / (num_episodes//2)
        }
        
        # æ¢å¤åŸå§‹é¢„æµ‹æ–¹æ³•
        self.model.predict = original_predict
        
        # æ¯”è¾ƒç»“æœ
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•å¯¹æ¯”:")
        print(f"ç¡®å®šæ€§é¢„æµ‹:")
        print(f"  å¹³å‡å¥–åŠ±: {results['deterministic']['stats']['reward_stats']['mean']:.2f}")
        print(f"  å®Œæˆç‡: {results['deterministic']['stats']['completion_rate']:.1%}")
        print(f"  æ¯å›åˆè€—æ—¶: {results['deterministic']['avg_time_per_episode']:.2f}s")
        
        print(f"éšæœºæ€§é¢„æµ‹:")
        print(f"  å¹³å‡å¥–åŠ±: {results['stochastic']['stats']['reward_stats']['mean']:.2f}")
        print(f"  å®Œæˆç‡: {results['stochastic']['stats']['completion_rate']:.1%}")
        print(f"  æ¯å›åˆè€—æ—¶: {results['stochastic']['avg_time_per_episode']:.2f}s")
        
        return results
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.env:
            self.env.close()

def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹')
    parser.add_argument('model_path', help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--episodes', '-e', type=int, default=10, 
                       help='æµ‹è¯•å›åˆæ•° (é»˜è®¤: 10)')
    parser.add_argument('--max-steps', '-s', type=int, default=1000,
                       help='æ¯å›åˆæœ€å¤§æ­¥æ•° (é»˜è®¤: 1000)')
    parser.add_argument('--render', '-r', action='store_true',
                       help='æ¸²æŸ“æ¸¸æˆç”»é¢')
    parser.add_argument('--benchmark', '-b', action='store_true',
                       help='è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--output', '-o', type=str,
                       help='æµ‹è¯•ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTester(args.model_path, render=args.render)
    
    try:
        if args.benchmark:
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            benchmark_results = tester.benchmark_performance(num_episodes=args.episodes)
            
            # ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
            if args.output:
                benchmark_output = args.output.replace('.json', '_benchmark.json')
                with open(benchmark_output, 'w') as f:
                    json.dump(benchmark_results, f, indent=2)
                print(f"ğŸ“ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {benchmark_output}")
        else:
            # æ ‡å‡†æµ‹è¯•
            summary_stats = tester.run_multiple_episodes(
                num_episodes=args.episodes,
                max_steps=args.max_steps
            )
            
            # æ‰“å°æŠ¥å‘Š
            tester.print_test_report(summary_stats)
            
            # ä¿å­˜ç»“æœ
            if args.output:
                tester.save_test_results(summary_stats, args.output)
            else:
                tester.save_test_results(summary_stats)
    
    finally:
        tester.close()

if __name__ == "__main__":
    main()