#!/usr/bin/env python3
"""
æ¨¡å‹è¡Œä¸ºè¯Šæ–­è„šæœ¬
æ·±å…¥åˆ†ææ¨¡å‹çš„å…·ä½“è¡Œä¸ºæ¨¡å¼ï¼Œæ‰¾å‡ºæ•ˆç‡ä½ä¸‹çš„åŸå› 
"""

import gym
import crafter
import env_wrapper
from model_with_attn import CustomPPO
import numpy as np
import time
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import json

class ModelBehaviorDiagnostic:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.action_history = []
        self.reward_history = []
        self.position_history = []
        self.direction_predictions = []
        self.behavior_patterns = defaultdict(int)
        
        self._setup_environment()
        self._load_model()
    
    def _setup_environment(self):
        """è®¾ç½®è¯Šæ–­ç¯å¢ƒ"""
        self.env = gym.make("MyCrafter-v0")
        self.env = env_wrapper.MineStoneWrapper(self.env)
        self.env = env_wrapper.InitWrapper(self.env, init_items=["wood_pickaxe"], init_num=[1])
        self.env = env_wrapper.DirectionLabelWrapper(self.env, target_obj_id=7, target_obj_name="stone")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = CustomPPO.load(self.model_path, env=self.env)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def analyze_single_episode(self, max_steps=300, verbose=True):
        """è¯¦ç»†åˆ†æå•ä¸ªå›åˆçš„è¡Œä¸º"""
        obs = self.env.reset()
        done = False
        step = 0
        total_reward = 0
        
        episode_data = {
            'actions': [],
            'rewards': [],
            'positions': [],
            'observations': [],
            'step_analysis': []
        }
        
        if verbose:
            print(f"\nğŸ” å¼€å§‹è¯¦ç»†è¡Œä¸ºåˆ†æ...")
        
        while not done and step < max_steps:
            # è®°å½•å½“å‰çŠ¶æ€
            current_position = self._extract_agent_position(obs)
            
            # é¢„æµ‹åŠ¨ä½œ
            action, _states = self.model.predict(obs, deterministic=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            new_obs, reward, done, info = self.env.step(action)
            
            # è®°å½•æ•°æ®
            episode_data['actions'].append(action)
            episode_data['rewards'].append(reward)
            episode_data['positions'].append(current_position)
            episode_data['observations'].append(obs.copy())
            
            # åˆ†æè¿™ä¸€æ­¥
            step_info = self._analyze_step(step, action, reward, current_position, obs, new_obs)
            episode_data['step_analysis'].append(step_info)
            
            total_reward += reward
            step += 1
            obs = new_obs
            
            if verbose and step % 50 == 0:
                print(f"   æ­¥æ•°: {step}, ç´¯è®¡å¥–åŠ±: {total_reward:.2f}, å½“å‰ä½ç½®: {current_position}")
        
        # å›åˆç»“æŸåˆ†æ
        episode_summary = self._summarize_episode(episode_data, total_reward, step, done)
        
        if verbose:
            self._print_episode_analysis(episode_summary)
        
        return episode_data, episode_summary
    
    def _extract_agent_position(self, obs):
        """ä»è§‚å¯Ÿä¸­æå–æ™ºèƒ½ä½“ä½ç½® (ç®€åŒ–ç‰ˆæœ¬)"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ä½ç½®æå–ï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®ç¯å¢ƒå…·ä½“å®ç°
        # è¿™é‡Œè¿”å›è§‚å¯Ÿçš„æŸäº›ç‰¹å¾ä½œä¸ºä½ç½®çš„è¿‘ä¼¼
        return (obs.mean(), obs.std())  # ä½¿ç”¨è§‚å¯Ÿçš„ç»Ÿè®¡ç‰¹å¾ä½œä¸ºä½ç½®ä»£ç†
    
    def _analyze_step(self, step, action, reward, position, obs, new_obs):
        """åˆ†æå•æ­¥è¡Œä¸º"""
        return {
            'step': step,
            'action': action,
            'reward': reward,
            'position': position,
            'obs_change': np.sum(np.abs(new_obs - obs)),  # è§‚å¯Ÿå˜åŒ–ç¨‹åº¦
            'reward_type': 'positive' if reward > 0 else 'negative' if reward < 0 else 'zero'
        }
    
    def _summarize_episode(self, episode_data, total_reward, total_steps, completed):
        """æ€»ç»“å›åˆè¡¨ç°"""
        actions = episode_data['actions']
        rewards = episode_data['rewards']
        
        # åŠ¨ä½œåˆ†æ
        action_counts = Counter(actions)
        most_common_actions = action_counts.most_common(3)
        
        # å¥–åŠ±åˆ†æ
        positive_rewards = [r for r in rewards if r > 0]
        negative_rewards = [r for r in rewards if r < 0]
        zero_rewards = [r for r in rewards if r == 0]
        
        # æ•ˆç‡åˆ†æ
        reward_per_step = total_reward / total_steps if total_steps > 0 else 0
        
        # è¡Œä¸ºæ¨¡å¼åˆ†æ
        behavior_patterns = self._identify_behavior_patterns(episode_data)
        
        return {
            'total_reward': total_reward,
            'total_steps': total_steps,
            'completed': completed,
            'reward_per_step': reward_per_step,
            'action_distribution': dict(action_counts),
            'most_common_actions': most_common_actions,
            'reward_breakdown': {
                'positive': len(positive_rewards),
                'negative': len(negative_rewards),
                'zero': len(zero_rewards),
                'pos_sum': sum(positive_rewards),
                'neg_sum': sum(negative_rewards)
            },
            'behavior_patterns': behavior_patterns,
            'efficiency_score': self._calculate_efficiency_score(total_reward, total_steps, completed)
        }
    
    def _identify_behavior_patterns(self, episode_data):
        """è¯†åˆ«è¡Œä¸ºæ¨¡å¼"""
        actions = episode_data['actions']
        patterns = {}
        
        # æ£€æŸ¥é‡å¤åŠ¨ä½œ
        consecutive_same = 0
        max_consecutive = 0
        for i in range(1, len(actions)):
            if actions[i] == actions[i-1]:
                consecutive_same += 1
                max_consecutive = max(max_consecutive, consecutive_same)
            else:
                consecutive_same = 0
        
        patterns['max_consecutive_same_action'] = max_consecutive
        
        # æ£€æŸ¥å¾ªç¯è¡Œä¸º (ç®€åŒ–ç‰ˆæœ¬)
        recent_actions = actions[-20:] if len(actions) >= 20 else actions
        patterns['recent_action_diversity'] = len(set(recent_actions)) / len(recent_actions) if recent_actions else 0
        
        # æ£€æŸ¥æ¢ç´¢ vs åˆ©ç”¨
        action_entropy = self._calculate_action_entropy(actions)
        patterns['action_entropy'] = action_entropy
        
        return patterns
    
    def _calculate_action_entropy(self, actions):
        """è®¡ç®—åŠ¨ä½œç†µï¼Œè¡¡é‡æ¢ç´¢ç¨‹åº¦"""
        if not actions:
            return 0
        action_counts = Counter(actions)
        total = len(actions)
        entropy = 0
        for count in action_counts.values():
            prob = count / total
            entropy -= prob * np.log2(prob)
        return entropy
    
    def _calculate_efficiency_score(self, reward, steps, completed):
        """è®¡ç®—æ•ˆç‡å¾—åˆ†"""
        if not completed:
            return 0.0
        
        # åŸºç¡€å®Œæˆåˆ†æ•°
        completion_score = 10.0
        
        # å¥–åŠ±æ•ˆç‡ (å¥–åŠ±è¶Šé«˜è¶Šå¥½)
        reward_score = max(0, reward) * 0.1
        
        # æ­¥æ•°æ•ˆç‡ (æ­¥æ•°è¶Šå°‘è¶Šå¥½)
        step_penalty = max(0, steps - 100) * 0.01
        
        efficiency = completion_score + reward_score - step_penalty
        return max(0, efficiency)
    
    def _print_episode_analysis(self, summary):
        """æ‰“å°å›åˆåˆ†æç»“æœ"""
        print(f"\n" + "="*60)
        print(f"ğŸ¯ å›åˆè¡Œä¸ºåˆ†ææŠ¥å‘Š")
        print(f"="*60)
        
        print(f"ğŸ“Š åŸºæœ¬æŒ‡æ ‡:")
        print(f"   æ€»å¥–åŠ±: {summary['total_reward']:.2f}")
        print(f"   æ€»æ­¥æ•°: {summary['total_steps']}")
        print(f"   å®ŒæˆçŠ¶æ€: {'âœ…' if summary['completed'] else 'âŒ'}")
        print(f"   æ•ˆç‡å¾—åˆ†: {summary['efficiency_score']:.2f}")
        print(f"   æ¯æ­¥å¥–åŠ±: {summary['reward_per_step']:.4f}")
        
        print(f"\nğŸ® åŠ¨ä½œåˆ†æ:")
        print(f"   æœ€å¸¸ç”¨åŠ¨ä½œ: {summary['most_common_actions']}")
        print(f"   åŠ¨ä½œå¤šæ ·æ€§: {len(summary['action_distribution'])} ç§ä¸åŒåŠ¨ä½œ")
        
        print(f"\nğŸ† å¥–åŠ±åˆ†æ:")
        rb = summary['reward_breakdown']
        print(f"   æ­£å¥–åŠ±æ­¥æ•°: {rb['positive']} (æ€»è®¡: {rb['pos_sum']:.2f})")
        print(f"   è´Ÿå¥–åŠ±æ­¥æ•°: {rb['negative']} (æ€»è®¡: {rb['neg_sum']:.2f})")
        print(f"   é›¶å¥–åŠ±æ­¥æ•°: {rb['zero']}")
        
        print(f"\nğŸ” è¡Œä¸ºæ¨¡å¼:")
        bp = summary['behavior_patterns']
        print(f"   æœ€å¤§è¿ç»­ç›¸åŒåŠ¨ä½œ: {bp['max_consecutive_same_action']}")
        print(f"   è¿‘æœŸåŠ¨ä½œå¤šæ ·æ€§: {bp['recent_action_diversity']:.3f}")
        print(f"   åŠ¨ä½œç†µ: {bp['action_entropy']:.3f}")
        
        # é—®é¢˜è¯Šæ–­
        print(f"\nâš ï¸ é—®é¢˜è¯Šæ–­:")
        issues = []
        
        if summary['reward_per_step'] < -0.03:
            issues.append("æ¯æ­¥å¥–åŠ±è¿‡ä½ï¼Œç­–ç•¥æ•ˆç‡éœ€è¦æ”¹è¿›")
        
        if bp['max_consecutive_same_action'] > 10:
            issues.append(f"å­˜åœ¨é‡å¤è¡Œä¸º (è¿ç»­{bp['max_consecutive_same_action']}æ¬¡ç›¸åŒåŠ¨ä½œ)")
        
        if bp['action_entropy'] < 1.0:
            issues.append("åŠ¨ä½œå¤šæ ·æ€§ä¸è¶³ï¼Œå¯èƒ½é™·å…¥å›ºå®šæ¨¡å¼")
        
        if rb['negative'] > rb['positive'] * 2:
            issues.append("è´Ÿå¥–åŠ±æ­¥æ•°è¿‡å¤šï¼Œè¡Œä¸ºé€‰æ‹©æœ‰é—®é¢˜")
        
        if not issues:
            issues.append("æœªå‘ç°æ˜æ˜¾é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´")
        
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
        
        print(f"="*60)

def main():
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python diagnose_model_behavior.py <æ¨¡å‹è·¯å¾„>")
        print("ç¤ºä¾‹: python diagnose_model_behavior.py ./stone_with_direction_v4_20240804_070000")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    print(f"ğŸ” å¼€å§‹è¯Šæ–­æ¨¡å‹è¡Œä¸º: {model_path}")
    
    diagnostic = ModelBehaviorDiagnostic(model_path)
    
    # åˆ†æ3ä¸ªå›åˆ
    for episode in range(3):
        print(f"\n{'='*20} å›åˆ {episode + 1} {'='*20}")
        episode_data, summary = diagnostic.analyze_single_episode(verbose=True)
        
        # ä¿å­˜è¯¦ç»†æ•°æ® (å¯é€‰)
        if episode == 0:  # åªä¿å­˜ç¬¬ä¸€å›åˆçš„è¯¦ç»†æ•°æ®
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            with open(f"behavior_analysis_{timestamp}.json", 'w') as f:
                # è½¬æ¢numpyæ•°ç»„ä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
                serializable_data = {
                    'summary': summary,
                    'action_sequence': episode_data['actions'],
                    'reward_sequence': episode_data['rewards']
                }
                json.dump(serializable_data, f, indent=2)
            print(f"ğŸ“ è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ° behavior_analysis_{timestamp}.json")

if __name__ == "__main__":
    main()